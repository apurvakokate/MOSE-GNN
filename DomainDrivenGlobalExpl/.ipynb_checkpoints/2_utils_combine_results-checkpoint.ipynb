{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bb5624-cdbe-4e0b-b520-0029d57863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import ast\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f412fc3-458b-4a09-a17e-8962cc3a2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dirs_dict = {\"esol\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/esol/65\",\n",
    "                             \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\",\n",
    "                         \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                 \"BBBP\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/BBBP/64\",\n",
    "                             \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\",\n",
    "                        \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                 \"Lipophilicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Lipophilicity/66\",\n",
    "                                  \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\",\n",
    "                                 \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                 \"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\",\n",
    "                                  \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\",\n",
    "                                \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                  \"hERG\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/hERG/69\",\n",
    "                                  \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\",\n",
    "                                \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bbdcfa-4269-4824-a09a-bffdc4a77543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dirs_dict = {\"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\",\n",
    "#                                   \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a02b2ce-f1b3-4660-8b69-6db8bea15664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped results saved to: ../kdd_results/results_summary_grouped_esol.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.434238   0.030430         0.464212  \\\n",
      "1      GATConv    None    0.455743   0.016453         0.470270   \n",
      "2      GATConv  RBRICS    0.407901   0.021031         0.438115   \n",
      "3      GCNConv   MGSSL    0.440523   0.031269         0.455010   \n",
      "4      GCNConv    None    0.455685   0.034852         0.477031   \n",
      "5      GCNConv  RBRICS    0.390118   0.014134         0.416379   \n",
      "6      GINConv   MGSSL    0.432184   0.048853         0.471568   \n",
      "7      GINConv    None    0.433375   0.032411         0.488646   \n",
      "8      GINConv  RBRICS    0.403754   0.042286         0.476240   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.042040   0.455019  0.046591  \n",
      "1        0.029702   0.469885  0.038958  \n",
      "2        0.032206   0.451754  0.050153  \n",
      "3        0.046624   0.453239  0.045117  \n",
      "4        0.034424   0.465019  0.050968  \n",
      "5        0.034225   0.422046  0.047931  \n",
      "6        0.027581   0.476054  0.040693  \n",
      "7        0.036864   0.496004  0.034203  \n",
      "8        0.041968   0.474632  0.075987  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_BBBP.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.877877   0.006234         0.846031  \\\n",
      "1      GATConv    None    0.871341   0.026457         0.851328   \n",
      "2      GATConv  RBRICS    0.904291   0.012142         0.867028   \n",
      "3      GCNConv   MGSSL    0.865205   0.038248         0.836682   \n",
      "4      GCNConv    None    0.832025   0.039368         0.818486   \n",
      "5      GCNConv  RBRICS    0.887925   0.053703         0.858162   \n",
      "6      GINConv   MGSSL    0.922676   0.019257         0.882940   \n",
      "7      GINConv    None    0.897334   0.029802         0.867652   \n",
      "8      GINConv  RBRICS    0.935077   0.009649         0.886514   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.031085   0.873411  0.028266  \n",
      "1        0.028628   0.858489  0.061453  \n",
      "2        0.032707   0.867643  0.037814  \n",
      "3        0.063130   0.867243  0.042326  \n",
      "4        0.072519   0.831699  0.038587  \n",
      "5        0.077961   0.865027  0.030767  \n",
      "6        0.043894   0.897349  0.026799  \n",
      "7        0.048032   0.870197  0.038328  \n",
      "8        0.033057   0.883030  0.037810  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_Lipophilicity.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.764187   0.010074         0.783406  \\\n",
      "1      GATConv    None    0.786291   0.012634         0.806873   \n",
      "2      GATConv  RBRICS    0.701027   0.017527         0.727413   \n",
      "3      GCNConv   MGSSL    0.753192   0.018659         0.771786   \n",
      "4      GCNConv    None    0.765452   0.008840         0.776981   \n",
      "5      GCNConv  RBRICS    0.702208   0.021252         0.723986   \n",
      "6      GINConv   MGSSL    0.689938   0.054496         0.726783   \n",
      "7      GINConv    None    0.696613   0.016007         0.732164   \n",
      "8      GINConv  RBRICS    0.640996   0.034889         0.707956   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.045848   0.782234  0.035812  \n",
      "1        0.045224   0.782220  0.044414  \n",
      "2        0.049688   0.726563  0.042137  \n",
      "3        0.045907   0.764276  0.042401  \n",
      "4        0.036412   0.760317  0.036280  \n",
      "5        0.040157   0.720102  0.047686  \n",
      "6        0.068407   0.752418  0.075850  \n",
      "7        0.029852   0.732894  0.042002  \n",
      "8        0.048325   0.708468  0.032933  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_Mutagenicity.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.837292   0.007163         0.797144  \\\n",
      "1      GATConv    None    0.765197   0.027184         0.734550   \n",
      "2      GATConv  RBRICS    0.859868   0.015472         0.807930   \n",
      "3      GCNConv   MGSSL    0.798012   0.007437         0.760964   \n",
      "4      GCNConv    None    0.769642   0.024070         0.740424   \n",
      "5      GCNConv  RBRICS    0.864932   0.007800         0.817602   \n",
      "6      GINConv   MGSSL    0.877189   0.004663         0.839923   \n",
      "7      GINConv    None    0.868088   0.006214         0.840009   \n",
      "8      GINConv  RBRICS    0.897783   0.003846         0.850584   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.022253   0.804340  0.020391  \n",
      "1        0.033346   0.760860  0.043071  \n",
      "2        0.031838   0.809319  0.026908  \n",
      "3        0.015118   0.775669  0.015347  \n",
      "4        0.024258   0.763616  0.041002  \n",
      "5        0.028893   0.815088  0.019176  \n",
      "6        0.007671   0.847497  0.014378  \n",
      "7        0.015567   0.848049  0.010086  \n",
      "8        0.018303   0.853890  0.015623  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_hERG.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.760739   0.000525         0.744460  \\\n",
      "1      GATConv    None    0.720091   0.021565         0.704992   \n",
      "2      GATConv  RBRICS    0.809561   0.009122         0.768616   \n",
      "3      GCNConv   MGSSL    0.751258   0.003469         0.736260   \n",
      "4      GCNConv    None    0.716438   0.020346         0.703844   \n",
      "5      GCNConv  RBRICS    0.806743   0.013380         0.768262   \n",
      "6      GINConv   MGSSL    0.801573   0.009647         0.764613   \n",
      "7      GINConv    None    0.794640   0.016909         0.765167   \n",
      "8      GINConv  RBRICS    0.845046   0.008092         0.789779   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.001659   0.737168  0.009483  \n",
      "1        0.027826   0.718262  0.035095  \n",
      "2        0.011557   0.765159  0.017628  \n",
      "3        0.019570   0.743297  0.017311  \n",
      "4        0.016052   0.716047  0.020777  \n",
      "5        0.009245   0.767559  0.017550  \n",
      "6        0.009465   0.764928  0.010393  \n",
      "7        0.012379   0.764843  0.015467  \n",
      "8        0.007003   0.776424  0.020230  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pdb\n",
    "\n",
    "# root_dir = \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/58\"\n",
    "# root_dirs_dict = {\"Lipophilicity\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/60\",\n",
    "#                               \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/61\",\n",
    "#                               \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/63\"\n",
    "#                              ],\n",
    "#                       \"esol\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/60\",\n",
    "#                              \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/62\"]}\n",
    "\n",
    "\n",
    "# Architectures and types to check\n",
    "architectures = [\"GATConv\", \"GINConv\", \"GCNConv\"]\n",
    "types = [\"MGSSL\", \"RBRICS\",\"None\"]\n",
    "\n",
    "# Folder name pattern: EXPT-{number}R-{dataset}-{seed}-{fold}-{architecture}-{type}\n",
    "folder_pattern = re.compile(\n",
    "    r\"EXPT-\\d+[a-zA-Z]+-[a-zA-Z]+-SEED-\\d+-FOLD-\\d+-(GATConv|GINConv|GCNConv)-[^-]+-(MGSSL|RBRICS)\"\n",
    ")\n",
    "\n",
    "folder_pattern_vanilla = re.compile(\n",
    "    r\"EXPT-\\d+[a-zA-Z]+-[a-zA-Z]+-SEED-\\d+-FOLD-\\d+-(GATConv|GINConv|GCNConv)-[^-]+-(None)?\"\n",
    ")\n",
    "\n",
    "for dataset_name, list_dirs in root_dirs_dict.items():\n",
    "    # Initialize the results dictionary\n",
    "    results = {\n",
    "        \"Architecture\": [],\n",
    "        \"Type\": [],\n",
    "        \"Train Metric\": [],\n",
    "        \"Validation Metric\": [],\n",
    "        \"Test Metric\": [],\n",
    "    }\n",
    "    \n",
    "\n",
    "    for root_dir in list_dirs:\n",
    "        # Traverse the directory and process matching folders\n",
    "        for folder in os.listdir(root_dir):\n",
    "            if folder_pattern.match(folder) or folder_pattern_vanilla.match(folder):\n",
    "                # Extract architecture and type from the folder name\n",
    "                match = folder_pattern.match(folder)\n",
    "                if match is not None:\n",
    "                    architecture, model_type = match.groups()\n",
    "                else:\n",
    "                    match = folder_pattern_vanilla.match(folder)\n",
    "                    architecture, model_type = match.groups()\n",
    "\n",
    "                if architecture in architectures and model_type in types:\n",
    "                    # Check for the JSON file\n",
    "                    file_path = os.path.join(root_dir, folder, f\"{dataset_name}_classification_result.json\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        with open(file_path, \"r\") as f:\n",
    "                            data = json.load(f)\n",
    "                        \n",
    "\n",
    "                        # Determine the appropriate metrics based on dataset_name\n",
    "                        if dataset_name in [\"BBBP\",\"Mutagenicity\",\"hERG\"]:\n",
    "                            train_metric = data.get(\"Trained_explainations_train_rocauc\", 0)\n",
    "                            validation_metric = data.get(\"Trained_explainations_validation_rocauc\", 0)\n",
    "                            test_metric = data.get(\"Trained_explainations_test_rocauc\", 0)\n",
    "                        else:\n",
    "                            train_metric = data.get(\"Trained_explainations_train_rmse\", 0)\n",
    "                            validation_metric = data.get(\"Trained_explainations_validation_rmse\", 0)\n",
    "                            test_metric = data.get(\"Trained_explainations_test_rmse\", 0)\n",
    "\n",
    "                        # Append single values for the current folder\n",
    "                        results[\"Architecture\"].append(architecture)\n",
    "                        results[\"Type\"].append(model_type)\n",
    "                        results[\"Train Metric\"].append(train_metric)\n",
    "                        results[\"Validation Metric\"].append(validation_metric)\n",
    "                        results[\"Test Metric\"].append(test_metric)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    df.to_csv(f\"../kdd_results/results_summary_{dataset_name}.csv\", index=False)\n",
    "\n",
    "    # Group by Architecture and Type, calculate mean and standard deviation\n",
    "    grouped = df.groupby([\"Architecture\", \"Type\"]).agg(\n",
    "        Train_Mean=(\"Train Metric\", \"mean\"),\n",
    "        Train_Std=(\"Train Metric\", \"std\"),\n",
    "        Validation_Mean=(\"Validation Metric\", \"mean\"),\n",
    "        Validation_Std=(\"Validation Metric\", \"std\"),\n",
    "        Test_Mean=(\"Test Metric\", \"mean\"),\n",
    "        Test_Std=(\"Test Metric\", \"std\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = f\"../kdd_results/results_summary_grouped_{dataset_name}.csv\"\n",
    "    grouped.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(\"Grouped results saved to:\", output_csv)\n",
    "    print(grouped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d11446b-da9f-40de-bd99-98b22832c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dirs_dict = {\"esol\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/esol/65\"],\n",
    "#                  \"BBBP\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/BBBP/64\"],\n",
    "#                  \"Lipophilicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Lipophilicity/66\"],\n",
    "#                  \"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21bb15cf-e344-4fd9-8bba-102920553a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF saved: ../kdd_results/esol_counts_Trueplots.pdf\n",
      "PDF saved: ../kdd_results/BBBP_counts_Trueplots.pdf\n",
      "PDF saved: ../kdd_results/Lipophilicity_counts_Trueplots.pdf\n",
      "PDF saved: ../kdd_results/Mutagenicity_counts_Trueplots.pdf\n",
      "PDF saved: ../kdd_results/hERG_counts_Trueplots.pdf\n"
     ]
    }
   ],
   "source": [
    "# Show casing the importanve vs logit diff per epoch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pdb\n",
    "import torch\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from DataLoader import get_setup_files_with_folds\n",
    "\n",
    "# root_dir = \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/58\"\n",
    "# root_dirs_dict = {\"Lipophilicity\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/61\",\n",
    "#                               \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/63\"\n",
    "#                              ],\n",
    "#                       \"esol\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/62\"]}\n",
    "\n",
    "# Architectures and types to check\n",
    "architectures = [\"GATConv\", \"GINConv\", \"GCNConv\"]\n",
    "types = [\"MGSSL\", \"RBRICS\"]\n",
    "\n",
    "# Define folder name patterns\n",
    "folder_pattern = re.compile(\n",
    "    r\"EXPT-\\d+[a-zA-Z]+-[a-zA-Z]+-SEED-(\\d+)-FOLD-(\\d+)-(GATConv|GINConv|GCNConv)-[^-]+-(MGSSL|RBRICS)\"\n",
    ")\n",
    "\n",
    "date_tag = '1225'\n",
    "use_count = True\n",
    "\n",
    "# Loop through datasets and directories\n",
    "for dataset_name, list_dirs in root_dirs_dict.items():\n",
    "    pdf_path = f\"../kdd_results/{dataset_name}_counts_{use_count}plots.pdf\"\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        root_dir = list_dirs[0]\n",
    "        # for root_dir in list_dirs:\n",
    "        # Dictionary to track data availability\n",
    "        plot_data = {arch: {type_: {} for type_ in types} for arch in architectures}\n",
    "\n",
    "        # Traverse the directory and process matching folders\n",
    "        for folder in os.listdir(root_dir):\n",
    "            if folder_pattern.match(folder):\n",
    "                # Extract architecture, type, seed, and fold from the folder name\n",
    "                match = folder_pattern.match(folder)\n",
    "                seed, fold, architecture, model_type = match.groups()\n",
    "\n",
    "                if architecture in architectures and model_type in types:\n",
    "                    # Check for the CSV file\n",
    "                    file_path = os.path.join(root_dir, folder, f\"explainer/{dataset_name}.csv\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        # Read the CSV file\n",
    "                        df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "                        # Process `Logit Diff` and `Motif params`\n",
    "                        df['Logit Diff'] = df['Logit Diff'].apply(ast.literal_eval)\n",
    "                        df['Motif params'] = df['Motif params'].apply(lambda x: torch.sigmoid(torch.tensor(ast.literal_eval(x))).tolist())\n",
    "\n",
    "                        # Fetch `motif_lengths` from setup files\n",
    "                        _, _, motif_counts, motif_lengths, _, _, _, _, _, _, _ = get_setup_files_with_folds(dataset_name, date_tag, fold, model_type)\n",
    "\n",
    "\n",
    "\n",
    "                       # Store data for plotting by seed and fold\n",
    "                        plot_data[architecture][model_type].setdefault(seed, {})[fold] = (df, motif_lengths,motif_counts)\n",
    "\n",
    "            \n",
    "            # Rename architectures\n",
    "            architecture_names = {\"GATConv\": \"GAT\", \"GINConv\": \"GIN\", \"GCNConv\": \"GCN\"}\n",
    "\n",
    "            # Iterate over seeds and folds for plotting\n",
    "            for seed in [0]:\n",
    "                for fold in range(5):  # Assuming 0-4 folds\n",
    "                    # Set up the plot\n",
    "                    fig, axes = plt.subplots(2, len(architectures), figsize=(20, 10))\n",
    "                    fig.suptitle(f\"{dataset_name} - Seed {seed} Fold {fold}\", fontsize=16)\n",
    "                    plt.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "                    \n",
    "                    avg_motifs = {}\n",
    "\n",
    "                    # Plot MGSSL and RDBRICS on separate rows\n",
    "                    for i, arch in enumerate(architectures):\n",
    "                        for j, model_type in enumerate(types):\n",
    "                            ax = axes[j, i]\n",
    "                            # pdb.set_trace()\n",
    "                            plot_data_entry = plot_data[arch][model_type].get(str(seed), {}).get(str(fold), None)\n",
    "                            if plot_data_entry is not None:\n",
    "                                df, motif_lengths,motif_counts = plot_data_entry\n",
    "                                # pdb.set_trace()\n",
    "                                num_motifs = len(df['Motif params'][0])\n",
    "                                for motif_idx in range(num_motifs):\n",
    "                                    try:\n",
    "                                        # Extract start, trajectory, and end points\n",
    "                                        start_x, start_y = df['Motif params'][0][motif_idx], df['Logit Diff'][0][motif_idx]\n",
    "                                        end_x, end_y = df['Motif params'].iloc[-1][motif_idx], df['Logit Diff'].iloc[-1][motif_idx]\n",
    "                                        trajectory = [\n",
    "                                            (epoch[motif_idx], diff[motif_idx])\n",
    "                                            for epoch, diff in zip(df['Motif params'], df['Logit Diff']) if len(epoch) > motif_idx\n",
    "                                        ]\n",
    "                                        traj_x, traj_y = zip(*trajectory)\n",
    "                                        \n",
    "                                        if use_count:\n",
    "                                            # Highlight start and end points with color based on motif length\n",
    "                                            motif_count = list(motif_counts.values())[motif_idx]\n",
    "                                            color = plt.cm.tab20c(motif_count / max(list(motif_counts.values())))\n",
    "                                        else:\n",
    "                                            # Highlight start and end points with color based on motif length\n",
    "                                            motif_length = list(motif_lengths.values())[motif_idx]\n",
    "                                            color = plt.cm.winter(motif_length / max(list(motif_lengths.values())))\n",
    "\n",
    "\n",
    "                                        # Plot trajectory\n",
    "                                        ax.plot(traj_x, traj_y, color=color, alpha=0.7)\n",
    "                                        \n",
    "                                        # Ensure points are visible with a minimum size\n",
    "                                        point_size = 50\n",
    "                                        \n",
    "                                        ax.scatter([start_x], [start_y], color='green', s=20)\n",
    "                                        ax.scatter([end_x], [end_y], color=color, s=20)\n",
    "\n",
    "                                        # Plot start and end points with color\n",
    "                                        # ax.scatter([start_x], [start_y], color=color, s=point_size, edgecolor='black', label=\"Start\")\n",
    "                                        # ax.scatter([end_x], [end_y], color=color, s=point_size, edgecolor='black', label=\"End\")\n",
    "\n",
    "                                        # ax.scatter([start_x], [start_y], color='green', s=point_size, label=\"Start\")\n",
    "                                        # ax.scatter([end_x], [end_y], color=color, s=point_size, edgecolor='black', label=\"End\")\n",
    "                                        # ax.scatter([start_x], [start_y], color='green', s=20, label=\"Start\")\n",
    "                                        # ax.scatter([end_x], [end_y], color='red', s=20, label=\"End\")\n",
    "                                        \n",
    "                                    except (IndexError, KeyError):\n",
    "                                        continue\n",
    "                                        \n",
    "                                # Add epoch count annotation\n",
    "                                num_epochs = len(df)\n",
    "                                ax.text(0.35, 0.15, f\"Epochs: {num_epochs}\",\n",
    "                                        transform=ax.transAxes, fontsize=10,\n",
    "                                        ha='right', va='top', bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "                                # Set plot labels\n",
    "                                ax.set_title(f\"{architecture_names[arch]} - {model_type}\", fontsize=12)\n",
    "                                ax.set_xlabel(\"Motif Parameters\")\n",
    "                                ax.set_ylabel(\"Old Logit - Logit after removal\")\n",
    "                                ax.set_xlim(0, 1)  # Set x-axis between 0 and 1\n",
    "                                ax.grid(True)\n",
    "                            else:\n",
    "                                # Add blank plot for missing data\n",
    "                                ax.set_title(f\"{architecture_names[arch]} - {model_type}\", fontsize=12)\n",
    "                                ax.text(0.5, 0.5, \"No Data\", fontsize=16, ha='center', va='center', color='gray')\n",
    "                                ax.set_xticks([])\n",
    "                                ax.set_yticks([])\n",
    "                                ax.set_frame_on(False)\n",
    "                    if use_count:\n",
    "                        # Add colorbar for end points (shared across plots)\n",
    "                        sm = plt.cm.ScalarMappable(cmap='tab20c', norm=plt.Normalize(vmin=min(list(motif_counts.values())), vmax=max(list(motif_counts.values()))))\n",
    "                        sm.set_array([])\n",
    "                        fig.colorbar(sm, ax=axes[:, :], orientation='vertical', label='Motif Frequency')\n",
    "                    else:\n",
    "                        # Add colorbar for end points (shared across plots)\n",
    "                        sm = plt.cm.ScalarMappable(cmap='winter', norm=plt.Normalize(vmin=min(list(motif_lengths.values())), vmax=max(list(motif_lengths.values()))))\n",
    "                        sm.set_array([])\n",
    "                        fig.colorbar(sm, ax=axes[:, :], orientation='vertical', label='Motif Length')\n",
    "\n",
    "\n",
    "                    # Save the figure to the PDF\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "            break\n",
    "\n",
    "    print(f\"PDF saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Loop through datasets and directories\n",
    "# for dataset_name, list_dirs in root_dirs_dict.items():\n",
    "#     for root_dir in list_dirs:\n",
    "#         # Traverse the directory and process matching folders\n",
    "#         for folder in os.listdir(root_dir):\n",
    "#             if folder_pattern.match(folder):\n",
    "#                 # Extract architecture and type from the folder name\n",
    "#                 match = folder_pattern.match(folder)\n",
    "\n",
    "#                 # Extract information\n",
    "#                 seed, fold, architecture, model_type = match.groups()\n",
    "\n",
    "#                 if architecture in architectures and model_type in types:\n",
    "#                     # Check for the CSV file\n",
    "#                     file_path = os.path.join(root_dir, folder, f\"explainer/{dataset_name}.csv\")\n",
    "#                     if os.path.exists(file_path):\n",
    "#                         # Read the CSV file\n",
    "#                         df = pd.read_csv(file_path)\n",
    "\n",
    "#                         # Process `Logit Diff` and `Motif params`\n",
    "#                         df['Logit Diff'] = df['Logit Diff'].apply(ast.literal_eval)\n",
    "#                         df['Motif params'] = df['Motif params'].apply(ast.literal_eval)\n",
    "\n",
    "#                         # Plot the data\n",
    "#                         num_epochs = len(df)\n",
    "#                         num_motifs = len(df['Motif params'][0])  # Assuming all rows have consistent motif counts\n",
    "\n",
    "#                         plt.figure(figsize=(12, 8))\n",
    "\n",
    "#                         for motif_idx in range(num_motifs):\n",
    "#                             try:\n",
    "#                                 # Extract start, trajectory, and end points for each motif\n",
    "#                                 start_x, start_y = df['Motif params'][0][motif_idx], df['Logit Diff'][0][motif_idx]\n",
    "#                                 end_x, end_y = df['Motif params'].iloc[-1][motif_idx], df['Logit Diff'].iloc[-1][motif_idx]\n",
    "#                                 trajectory = [\n",
    "#                                     (epoch[motif_idx], diff[motif_idx])\n",
    "#                                     for epoch, diff in zip(df['Motif params'], df['Logit Diff']) if len(epoch) > motif_idx\n",
    "#                                 ]\n",
    "\n",
    "#                                 # Extract X and Y coordinates for trajectory\n",
    "#                                 traj_x, traj_y = zip(*trajectory)\n",
    "\n",
    "#                                 # Plot trajectory\n",
    "#                                 plt.plot(traj_x, traj_y, alpha=0.5, label=f\"Motif {motif_idx} Trajectory\")\n",
    "\n",
    "#                                 # Highlight start and end points\n",
    "#                                 plt.scatter([start_x], [start_y], color='green', label=f\"Motif {motif_idx} Start\")\n",
    "#                                 plt.scatter([end_x], [end_y], color='red', label=f\"Motif {motif_idx} End\")\n",
    "#                             except (IndexError, KeyError):\n",
    "#                                 print(f\"Error processing motif {motif_idx}, skipping...\")\n",
    "#                                 continue\n",
    "\n",
    "#                         # Enhance visualization\n",
    "#                         plt.title(f\"Motif Trajectories for {dataset_name} ({architecture} - {model_type})\")\n",
    "#                         plt.xlabel(\"Motif Parameters\")\n",
    "#                         plt.ylabel(\"Old Logit - New logit (Motif removed)\")\n",
    "#                         # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "#                         plt.tight_layout()\n",
    "\n",
    "#                         # Show the plot\n",
    "#                         plt.show()\n",
    "# for dataset_name, list_dirs in root_dirs_dict.items():\n",
    "#     for root_dir in list_dirs:\n",
    "#         # Traverse the directory and process matching folders\n",
    "#         for folder in os.listdir(root_dir):\n",
    "#             if folder_pattern.match(folder) :\n",
    "#                 # Extract architecture and type from the folder name\n",
    "#                 match = folder_pattern.match(folder)\n",
    "\n",
    "#                 # Extract information\n",
    "#                 seed, fold, architecture, model_type = match.groups()\n",
    "\n",
    "#                 if architecture in architectures and model_type in types:\n",
    "#                     # Check for the CSV file\n",
    "#                     file_path = os.path.join(root_dir, folder, f\"explainer/{dataset_name}.csv\")\n",
    "#                     if os.path.exists(file_path):\n",
    "#                         # Read the CSV file\n",
    "#                         df = pd.read_csv(file_path)\n",
    "                        \n",
    "#                         # Process `Logit Diff` and `Motif params`\n",
    "#                         df['Logit Diff'] = df['Logit Diff'].apply(ast.literal_eval)\n",
    "#                         df['Motif params'] = df['Motif params'].apply(ast.literal_eval)\n",
    "\n",
    "#                         # Flatten the `Logit Diff` and `Motif params` for plotting\n",
    "#                         logit_diffs = []\n",
    "#                         motif_params = []\n",
    "#                         for _, row in df.iterrows():\n",
    "#                             logit_diffs.extend(row['Logit Diff'].values())\n",
    "#                             motif_params.extend(row['Motif params'])\n",
    "\n",
    "#                         # Plot the data\n",
    "#                         plt.figure(figsize=(10, 6))\n",
    "#                         plt.scatter(motif_params, logit_diffs, alpha=0.6, edgecolors='w')\n",
    "#                         plt.xlabel('Motif Parameters')\n",
    "#                         plt.ylabel('Logit Diff')\n",
    "#                         plt.title(f'Logit Diff vs Motif Parameters ({dataset_name} - {architecture} - {model_type})')\n",
    "#                         plt.grid(True)\n",
    "#                         plt.show()\n",
    "                            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a13f590-98d9-47f1-a6b7-0a6e984e7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_results_summary(folder_path):\n",
    "    # Initialize a list to store processed data for all datasets\n",
    "    processed_data = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file matches the required pattern\n",
    "        if file_name.startswith(\"results_summary_grouped_\") and file_name.endswith(\".csv\"):\n",
    "            # Extract the dataset name from the file name\n",
    "            dataset_name = file_name[len(\"results_summary_grouped_\"):-len(\".csv\")]\n",
    "\n",
    "            # Read the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Replace 'None' with 'Vanilla' in the 'Type' column\n",
    "            df['Type'] = df['Type'].fillna('Vanilla')\n",
    "            \n",
    "            # Replace architecture values\n",
    "            df['Architecture'] = df['Architecture'].replace({\n",
    "                \"GATConv\": \"GAT\",\n",
    "                \"GINConv\": \"GIN\",\n",
    "                \"GCNConv\": \"GCN\"\n",
    "            })\n",
    "\n",
    "            # Format Train, Validation, and Test metrics to 0.3f precision\n",
    "            df['Train Metric'] = df['Train_Mean'].map(\"{:.3f}\".format) + \" ± \" + df['Train_Std'].map(\"{:.3f}\".format)\n",
    "            df['Validation Metric'] = df['Validation_Mean'].map(\"{:.3f}\".format) + \" ± \" + df['Validation_Std'].map(\"{:.3f}\".format)\n",
    "            df['Test Metric'] = df['Test_Mean'].map(\"{:.3f}\".format) + \" ± \" + df['Test_Std'].map(\"{:.3f}\".format)\n",
    "\n",
    "            # Keep only relevant columns\n",
    "            df = df[['Architecture', 'Type', 'Train Metric', 'Validation Metric', 'Test Metric']]\n",
    "\n",
    "            # Add the dataset name to the DataFrame\n",
    "            df.insert(0, 'Dataset Name', dataset_name)\n",
    "            \n",
    "            # Sort the DataFrame by 'Type' with 'Vanilla' first\n",
    "            df['Type'] = pd.Categorical(df['Type'], categories=['Vanilla', 'MGSSL','RBRICS'], ordered=True)\n",
    "            df['Architecture'] = pd.Categorical(df['Architecture'], categories=['GAT', 'GCN','GIN'], ordered=True)\n",
    "            \n",
    "            df = df.sort_values(by=['Dataset Name', 'Architecture', 'Type'])\n",
    "            \n",
    "            \n",
    "\n",
    "            # Append to the processed data list\n",
    "            processed_data.append(df)\n",
    "\n",
    "    # Concatenate all processed data into a single DataFrame\n",
    "    final_df = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    # Split the data into two groups based on the dataset\n",
    "    group_1_datasets = [\"esol\", \"Lipophilicity\"]\n",
    "    group_2_datasets = [\"Mutagenicity\", \"BBBP\", \"hERG\"]\n",
    "\n",
    "    group_1_df = final_df[final_df['Dataset Name'].isin(group_1_datasets)].copy()\n",
    "    group_2_df = final_df[final_df['Dataset Name'].isin(group_2_datasets)].copy()\n",
    "\n",
    "    # Function to generate LaTeX table for a group\n",
    "    def generate_latex_table(dataframe, output_file):\n",
    "        # Remove duplicate dataset names for multirow formatting\n",
    "        dataframe.loc[:, 'Dataset Name'] = dataframe['Dataset Name'].mask(dataframe['Dataset Name'].duplicated(), '')\n",
    "\n",
    "        # Convert DataFrame to LaTeX table\n",
    "        latex_code = dataframe.to_latex(index=False, escape=False, column_format=\"|l|l|l|l|l|l|\", header=[\n",
    "            \"Dataset Name\", \"GNN Architecture\", \"Model\", \"Train Metric\", \"Validation Metric\", \"Test Metric\"\n",
    "        ])\n",
    "        \n",
    "        # Add table borders and formatting\n",
    "        latex_code = latex_code.replace('\\\\toprule', '\\\\hline').replace('\\\\midrule', '').replace('\\\\bottomrule', '\\\\hline')\n",
    "\n",
    "        # Write the LaTeX code to a file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(latex_code)\n",
    "\n",
    "        print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "    # Generate LaTeX tables for the two groups\n",
    "    group_1_file = os.path.join(folder_path, \"results_summary_regression.tex\")\n",
    "    group_2_file = os.path.join(folder_path, \"results_summary_binary_classification.tex\")\n",
    "\n",
    "    generate_latex_table(group_1_df, group_1_file)\n",
    "    generate_latex_table(group_2_df, group_2_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9232678-656a-4f4d-b918-fb13f5f2a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to ../kdd_results/results_summary_regression.tex\n",
      "LaTeX table saved to ../kdd_results/results_summary_binary_classification.tex\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = \"../kdd_results\"  # Replace with the path to your folder\n",
    "process_results_summary(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb9de46-7d1f-458c-b0ab-4de82b1664c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Architecture    Type  Train Mean  Train Std  Validation Mean  \\\n",
      "0       GINConv  RBRICS    3.202261        NaN         3.244942   \n",
      "1       GCNConv  RBRICS    3.143503        NaN         3.360423   \n",
      "2       GATConv  RBRICS    3.133229        NaN         3.359058   \n",
      "3       GATConv  RBRICS    3.232798        NaN         2.991389   \n",
      "4       GINConv   MGSSL    3.215844        NaN         3.254110   \n",
      "5       GATConv   MGSSL    3.151217        NaN         3.366590   \n",
      "6       GCNConv   MGSSL    3.116649        NaN         3.503303   \n",
      "7       GCNConv   MGSSL    3.285963        NaN         3.162374   \n",
      "8       GCNConv  RBRICS    3.301915        NaN         3.179567   \n",
      "9       GATConv   MGSSL    3.274320        NaN         3.154436   \n",
      "10      GCNConv   MGSSL    3.281859        NaN         3.316725   \n",
      "11      GINConv   MGSSL    3.139627        NaN         3.531137   \n",
      "12      GINConv   MGSSL    3.217353        NaN         3.107682   \n",
      "13      GATConv   MGSSL    3.103157        NaN         3.505188   \n",
      "14      GINConv  RBRICS    3.217744        NaN         2.969827   \n",
      "15      GINConv  RBRICS    3.155842        NaN         3.367303   \n",
      "16      GATConv   MGSSL    3.226069        NaN         2.982112   \n",
      "17      GCNConv  RBRICS    3.132435        NaN         3.356549   \n",
      "18      GCNConv   MGSSL    3.148896        NaN         3.363416   \n",
      "19      GCNConv   MGSSL    3.222660        NaN         2.982025   \n",
      "20      GINConv  RBRICS    3.155854        NaN         3.387501   \n",
      "21      GATConv   MGSSL    3.215688        NaN         3.265003   \n",
      "22      GINConv  RBRICS    3.215226        NaN         3.103612   \n",
      "23      GCNConv  RBRICS    3.234901        NaN         2.986006   \n",
      "24      GCNConv  RBRICS    3.198787        NaN         3.246681   \n",
      "25      GATConv  RBRICS    3.294395        NaN         3.174339   \n",
      "26      GINConv   MGSSL    3.170889        NaN         3.380788   \n",
      "27      GINConv  RBRICS    3.138095        NaN         3.529131   \n",
      "28      GINConv   MGSSL    3.225464        NaN         2.980916   \n",
      "29      GCNConv  RBRICS    3.114923        NaN         3.501896   \n",
      "30      GATConv  RBRICS    0.000000        0.0         0.000000   \n",
      "31      GATConv  RBRICS    3.221257        NaN         3.268502   \n",
      "32      GATConv  RBRICS    3.103076        NaN         3.505079   \n",
      "\n",
      "    Validation Std  Test Mean  Test Std  \n",
      "0              NaN   3.107646       NaN  \n",
      "1              NaN   3.288083       NaN  \n",
      "2              NaN   3.287229       NaN  \n",
      "3              NaN   3.381818       NaN  \n",
      "4              NaN   3.121166       NaN  \n",
      "5              NaN   3.294880       NaN  \n",
      "6              NaN   3.303635       NaN  \n",
      "7              NaN   3.265019       NaN  \n",
      "8              NaN   3.283263       NaN  \n",
      "9              NaN   3.256309       NaN  \n",
      "10             NaN   3.189968       NaN  \n",
      "11             NaN   3.329772       NaN  \n",
      "12             NaN   3.208897       NaN  \n",
      "13             NaN   3.305316       NaN  \n",
      "14             NaN   3.372755       NaN  \n",
      "15             NaN   3.299458       NaN  \n",
      "16             NaN   3.374550       NaN  \n",
      "17             NaN   3.284681       NaN  \n",
      "18             NaN   3.291641       NaN  \n",
      "19             NaN   3.372211       NaN  \n",
      "20             NaN   3.320415       NaN  \n",
      "21             NaN   3.130061       NaN  \n",
      "22             NaN   3.201506       NaN  \n",
      "23             NaN   3.375948       NaN  \n",
      "24             NaN   3.109098       NaN  \n",
      "25             NaN   3.277528       NaN  \n",
      "26             NaN   3.313760       NaN  \n",
      "27             NaN   3.324261       NaN  \n",
      "28             NaN   3.371371       NaN  \n",
      "29             NaN   3.302022       NaN  \n",
      "30             0.0   0.000000       0.0  \n",
      "31             NaN   3.136743       NaN  \n",
      "32             NaN   3.304993       NaN  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Root directory containing the folders\n",
    "root_dir = \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/54\"\n",
    "\n",
    "# Initialize data structure to store results\n",
    "results = {\n",
    "    \"Architecture\": [],\n",
    "    \"Type\": [],\n",
    "    \"Train Mean\": [],\n",
    "    \"Train Std\": [],\n",
    "    \"Validation Mean\": [],\n",
    "    \"Validation Std\": [],\n",
    "    \"Test Mean\": [],\n",
    "    \"Test Std\": []\n",
    "}\n",
    "\n",
    "# Architectures and types to check\n",
    "architectures = [\"GATConv\", \"GINConv\", \"GCNConv\"]\n",
    "types = [\"MGSSL\", \"RBRICS\"]\n",
    "\n",
    "# Folder name pattern: EXPT-{number}R-{dataset}-{seed}-{fold}-{architecture}-{type}\n",
    "folder_pattern = re.compile(\n",
    "    r\"EXPT-\\d+R-[a-zA-Z]+-SEED-\\d+-FOLD-\\d+-(GATConv|GINConv|GCNConv)-[^-]+-(MGSSL|RBRICS)\"\n",
    ")\n",
    "# EXPT-12R-esol-SEED-0-FOLD-0-GATConv-SingleChannel-RBRICS\n",
    "# Traverse the directory and process matching folders\n",
    "for folder in os.listdir(root_dir):\n",
    "\n",
    "    if folder_pattern.match(folder):\n",
    "\n",
    "        # Extract architecture and type from the folder name\n",
    "        match = folder_pattern.match(folder)\n",
    "        architecture, model_type = match.groups()\n",
    "\n",
    "        if architecture in architectures and model_type in types:\n",
    "            train_maes, val_maes, test_maes = [], [], []\n",
    "\n",
    "            file_path = os.path.join(root_dir, folder, \"esol_classification_result.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                train_maes.append(data.get(\"Trained_explainations_train_mae\", 0))\n",
    "                val_maes.append(data.get(\"Trained_explainations_validation_mae\", 0))\n",
    "                test_maes.append(data.get(\"Trained_explainations_test_mae\", 0))\n",
    "\n",
    "            # Calculate statistics for the current architecture and type\n",
    "            train_mean = sum(train_maes) / len(train_maes) if train_maes else 0\n",
    "            train_std = pd.Series(train_maes).std() if train_maes else 0\n",
    "            val_mean = sum(val_maes) / len(val_maes) if val_maes else 0\n",
    "            val_std = pd.Series(val_maes).std() if val_maes else 0\n",
    "            test_mean = sum(test_maes) / len(test_maes) if test_maes else 0\n",
    "            test_std = pd.Series(test_maes).std() if test_maes else 0\n",
    "\n",
    "            # Append results\n",
    "            results[\"Architecture\"].append(architecture)\n",
    "            results[\"Type\"].append(model_type)\n",
    "            results[\"Train Mean\"].append(train_mean)\n",
    "            results[\"Train Std\"].append(train_std)\n",
    "            results[\"Validation Mean\"].append(val_mean)\n",
    "            results[\"Validation Std\"].append(val_std)\n",
    "            results[\"Test Mean\"].append(test_mean)\n",
    "            results[\"Test Std\"].append(test_std)\n",
    "\n",
    "# Create a DataFrame and display the results\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "merged_df = df.groupby([\"Architecture\", \"Type\"], as_index=False).agg({\n",
    "    \"Train Mean\": \"mean\",\n",
    "    \"Train Std\": \"mean\",  # Mean of stds across folds/seeds\n",
    "    \"Validation Mean\": \"mean\",\n",
    "    \"Validation Std\": \"mean\",\n",
    "    \"Test Mean\": \"mean\",\n",
    "    \"Test Std\": \"mean\"\n",
    "})\n",
    "# Optionally, save to a CSV file\n",
    "df.to_csv(\"results_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71da888c-660e-4fe4-8575-2a5fe8114e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(folder_pattern.match(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29516ab3-e8db-4d7f-910a-03058c0d6a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Change result dict when you run new experiments \n",
    "\n",
    "folder - Enter list of folders to check, each folder will be traversed to check if file exists in folder\n",
    "seed\n",
    "arch\n",
    "size_reg\n",
    "dataset\n",
    "\n",
    "**IMPORTANT** : Change EXPT_BASE_NAME if you update the folder name to where your results are stored\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    \n",
    "    'folder' : [38,39,40,41],\n",
    "    'seed' : [2,3],\n",
    "    'arch' : ['GINConv','GCNConv','GATConv'],\n",
    "    'size_reg' : [0,0.001,0.1],\n",
    "    'dataset'  : {'BBBP':'BBBP','MUTAG':'Mutagenicity','HERG':'hERG'}\n",
    "}\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    \n",
    "    'folder' : [27,29,30,31,32,33,34,35],\n",
    "    'seed' : [0,1],\n",
    "    'arch' : ['GINConv','GCNConv','GATConv'],\n",
    "    'size_reg' : [0,0.001,0.1],\n",
    "    'dataset'  : {'BBBP':'BBBP','MUTAG':'Mutagenicity','HERG':'hERG'}\n",
    "}\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    \n",
    "    'folder' : [42,43],\n",
    "    'seed' : [2,3],\n",
    "    'arch' : ['GINConv','GCNConv','GATConv'],\n",
    "    'channel' : ['Vanilla'],\n",
    "    'dataset'  : {'BBBP':'BBBP','MUTAG':'Mutagenicity'}\n",
    "}\n",
    "\n",
    "'''\n",
    "date_tag = '0830'\n",
    "\n",
    "\n",
    "expt_dict = [\n",
    "    \n",
    "    {\"dataset\":\"HERG\", \"seed_info\" : {1:49,3:49,0:48,2:48}},\n",
    "    {\"dataset\":\"MUTAG\", \"seed_info\" : {1:47,3:47,0:46,2:46}},\n",
    "    {\"dataset\":\"BBBP\", \"seed_info\" : {1:47,3:47,0:46,2:46}}]\n",
    "\n",
    "dataset_dict = {'BBBP':'BBBP','MUTAG':'Mutagenicity','HERG':'hERG'}\n",
    "#dataset_dict = {'BBBP':'BBBP'}\n",
    "\n",
    "def return_expt_base_name(seed,layer_type,channel,folder,dataset):\n",
    "\n",
    "    BASE_PATH = f'../Cluster_JOBS/LoG_plot/{folder}/EXPT-32A{dataset}-{seed}-{layer_type}-{channel}'\n",
    "    return BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bfb1c4-c369-4f96-b67f-fba583b66c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the Following Config to Final Results, copy them over to add to the plots pdf\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dictionary/BBBP_motif_list_0830.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_495609/1516760633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'dictionary/{dataset_dict[dataset]}_motif_list_{date_tag}.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmotif_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmotif_count_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dictionary/BBBP_motif_list_0830.pickle'"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "\n",
    "print(f'Adding the Following Config to Final Results, copy them over to add to the plots pdf')\n",
    "\n",
    "motif_count_dict = {}\n",
    "\n",
    "for dataset in dataset_dict.keys():\n",
    "    with open(f'dictionary/{dataset_dict[dataset]}_motif_list_{date_tag}.pickle', 'rb') as file:\n",
    "        motif_list = list(pickle.load(file))\n",
    "    if dataset not in motif_count_dict.keys():\n",
    "        motif_count_dict[dataset] = {}\n",
    "    \n",
    "    motif_count_dict[dataset]['model'] = {}\n",
    "    motif_count_dict[dataset]['logit'] = {}\n",
    "        \n",
    "        \n",
    "    for channel in [\"DualParam\",\"SingleChannel\"]:\n",
    "        for layer_type in [\"GINConv\", \"GCNConv\", \"GATConv\"]:\n",
    "            for seed in [0,1,2,3]:\n",
    "                \n",
    "                for folder_dict in expt_dict:\n",
    "                    if folder_dict['dataset'] == dataset:\n",
    "                        folder = folder_dict['seed_info'][seed]\n",
    "                \n",
    "                EXPT_PATH = return_expt_base_name(seed,layer_type,channel,folder,dataset)\n",
    "                \n",
    "                print(EXPT_PATH)\n",
    "                \n",
    "                found = True\n",
    "\n",
    "                if os.path.exists(EXPT_PATH):\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        #Recreate loss metrics\n",
    "                        logit_val = pd.read_csv(EXPT_PATH+f'/{dataset_dict[dataset]}_explanation_result_with_validation.csv')\n",
    "                        logit_test = pd.read_csv(EXPT_PATH+f'/{dataset_dict[dataset]}_explanation_result_with_test.csv')\n",
    "                    except Exception as e:\n",
    "                        print(f\"error when trying to validation motif logit differences: {e}\")\n",
    "                        found = False\n",
    "                    \n",
    "                    #validation\n",
    "                    logit_val['logit_diff_class_0'] = logit_val['new_logit_class_0'] - logit_val['original_logit_class_0']\n",
    "                    logit_val['logit_diff_class_1'] = logit_val['new_logit_class_1'] - logit_val['original_logit_class_1']\n",
    "                    \n",
    "                    #test\n",
    "                    logit_test['logit_diff_class_0'] = logit_test['new_logit_class_0'] - logit_test['original_logit_class_0']\n",
    "                    logit_test['logit_diff_class_1'] = logit_test['new_logit_class_1'] - logit_test['original_logit_class_1']\n",
    "                                        \n",
    "                    if channel == 'DualParam':\n",
    "                    \n",
    "                        #get mean scores for val\n",
    "                        logit_val = logit_val.groupby('motif_id').agg(\n",
    "                                                        mean_logit_diff_class_0   = ('logit_diff_class_0', 'mean'),\n",
    "                                                        mean_logit_diff_class_1   = ('logit_diff_class_1', 'mean'),\n",
    "                                                        mean_sig_imp_class_0      = ('sigmoid_importance_for_class_0', 'mean'),\n",
    "                                                        mean_sig_imp_class_1      = ('sigmoid_importance_for_class_1', 'mean')).reset_index()\n",
    "                        \n",
    "                        #get mean scores for test\n",
    "                        logit_test = logit_test.groupby('motif_id').agg(\n",
    "                                                        mean_logit_diff_class_0   = ('logit_diff_class_0', 'mean'),\n",
    "                                                        mean_logit_diff_class_1   = ('logit_diff_class_1', 'mean'),\n",
    "                                                        mean_sig_imp_class_0      = ('sigmoid_importance_for_class_0', 'mean'),\n",
    "                                                        mean_sig_imp_class_1      = ('sigmoid_importance_for_class_1', 'mean')).reset_index()\n",
    "                    else:                    \n",
    "                        #get mean scores for val\n",
    "                        logit_val = logit_val.groupby('motif_id').agg(\n",
    "                                                        mean_logit_diff_class_0   = ('logit_diff_class_0', 'mean'),\n",
    "                                                        mean_logit_diff_class_1   = ('logit_diff_class_1', 'mean'),\n",
    "                                                        mean_sig_imp_class_0      = ('sigmoid_importance_for_class_0', 'mean')).reset_index()\n",
    "                        \n",
    "                        #get mean scores for test\n",
    "                        logit_test = logit_test.groupby('motif_id').agg(\n",
    "                                                        mean_logit_diff_class_0   = ('logit_diff_class_0', 'mean'),\n",
    "                                                        mean_logit_diff_class_1   = ('logit_diff_class_1', 'mean'),\n",
    "                                                        mean_sig_imp_class_0      = ('sigmoid_importance_for_class_0', 'mean')).reset_index()\n",
    "                        \n",
    "                    \n",
    "                    # sort by logit_diff\n",
    "                    logit_0_motifs = logit_val.sort_values('mean_logit_diff_class_0', ascending = False).motif_id.tolist()[0:10]\n",
    "                    \n",
    "                    for motif in logit_0_motifs:\n",
    "                        if motif_list[motif] not in motif_count_dict[dataset]['logit'].keys():\n",
    "                            motif_count_dict[dataset]['logit'][motif_list[motif]] = 1\n",
    "                        else:\n",
    "                            motif_count_dict[dataset]['logit'][motif_list[motif]] += 1\n",
    "                    \n",
    "                    logit_1_motifs = logit_val.sort_values('mean_logit_diff_class_1', ascending = False).motif_id.tolist()[0:10]\n",
    "                    \n",
    "                    for motif in logit_1_motifs:\n",
    "                        if motif_list[motif] not in motif_count_dict[dataset]['logit'].keys():\n",
    "                            motif_count_dict[dataset]['logit'][motif_list[motif]] = 1\n",
    "                        else:\n",
    "                            motif_count_dict[dataset]['logit'][motif_list[motif]] += 1\n",
    "                            \n",
    "                    \n",
    "#                     #sort by model importance\n",
    "#                     model_0_motifs = logit_val.sort_values('mean_sig_imp_class_0', ascending = False).motif_id.tolist()[0:10]\n",
    "                    \n",
    "#                     for motif in model_0_motifs:\n",
    "#                         if motif_list[motif] not in motif_count_dict[dataset]['model'].keys():\n",
    "#                             motif_count_dict[dataset]['model'][motif_list[motif]] = 1\n",
    "#                         else:\n",
    "#                             motif_count_dict[dataset]['model'][motif_list[motif]] += 1\n",
    "                    \n",
    "#                     if channel == 'DualParam':\n",
    "#                         model_1_motifs = logit_val.sort_values('mean_sig_imp_class_1', ascending = False).motif_id.tolist()[0:10]\n",
    "                    \n",
    "#                         for motif in model_1_motifs:\n",
    "#                             if motif_list[motif] not in motif_count_dict[dataset]['model'].keys():\n",
    "#                                 motif_count_dict[dataset]['model'][motif_list[motif]] = 1\n",
    "#                             else:\n",
    "#                                 motif_count_dict[dataset]['model'][motif_list[motif]] += 1\n",
    "                    \n",
    "                            \n",
    "print(motif_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72d040-6808-4c9e-96e3-c8dc83a801d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(result_path,index_col = None)\n",
    "# df['model_type']='DualParam'\n",
    "# df.to_csv(result_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687cb27-eca7-40ba-8239-2d84690ad91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# Iterate through the dictionary\n",
    "for dataset, models in motif_count_dict.items():\n",
    "    for model, attributes in models.items():\n",
    "        for key, value in attributes.items():\n",
    "            # Append a tuple with the required structure\n",
    "            rows.append((dataset, model, key, value))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(rows, columns=['Dataset', 'Importance Criteria', 'Motif Name', 'Count'])\n",
    "\n",
    "df = df.sort_values(['Dataset', 'Importance Criteria', 'Count'], ascending = False)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('important_motif_count_logit.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f8fe0-cc2e-43e1-a2eb-9722a4a6f913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-test",
   "language": "python",
   "name": "py36-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
