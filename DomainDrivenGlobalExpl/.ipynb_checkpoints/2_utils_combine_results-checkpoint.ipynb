{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bb5624-cdbe-4e0b-b520-0029d57863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import ast\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f412fc3-458b-4a09-a17e-8962cc3a2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dirs_dict = {\"esol\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/esol/65\",\n",
    "                             \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Vanilla/85\",\n",
    "                         \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                 \"BBBP\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/BBBP/64\",\n",
    "                             \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Vanilla/85\",\n",
    "                        \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                 \"Lipophilicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Lipophilicity/66\",\n",
    "                                  \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Vanilla/85\",\n",
    "                                 \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                 \"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\",\n",
    "                                  \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Vanilla/85\",\n",
    "                                \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"],\n",
    "                  \"hERG\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/hERG/69\",\n",
    "                                  \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Vanilla/85\",\n",
    "                                \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/94\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bbdcfa-4269-4824-a09a-bffdc4a77543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dirs_dict = {\"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\",\n",
    "#                                   \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/85\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a02b2ce-f1b3-4660-8b69-6db8bea15664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped results saved to: ../kdd_results/results_summary_grouped_esol.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.434238   0.030430         0.464212  \\\n",
      "1      GATConv    None    0.447644   0.023044         0.470857   \n",
      "2      GATConv  RBRICS    0.407901   0.021031         0.438115   \n",
      "3      GCNConv   MGSSL    0.440523   0.031269         0.455010   \n",
      "4      GCNConv    None    0.455685   0.034852         0.477031   \n",
      "5      GCNConv  RBRICS    0.390118   0.014134         0.416379   \n",
      "6      GINConv   MGSSL    0.432184   0.048853         0.471568   \n",
      "7      GINConv    None    0.433375   0.032411         0.488646   \n",
      "8      GINConv  RBRICS    0.403754   0.042286         0.476240   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.042040   0.455019  0.046591  \n",
      "1        0.025756   0.460230  0.040055  \n",
      "2        0.032206   0.451754  0.050153  \n",
      "3        0.046624   0.453239  0.045117  \n",
      "4        0.034424   0.465019  0.050968  \n",
      "5        0.034225   0.422046  0.047931  \n",
      "6        0.027581   0.476054  0.040693  \n",
      "7        0.036864   0.496004  0.034203  \n",
      "8        0.041968   0.474632  0.075987  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_BBBP.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.877877   0.006234         0.846031  \\\n",
      "1      GATConv    None    0.871341   0.026457         0.851328   \n",
      "2      GATConv  RBRICS    0.904291   0.012142         0.867028   \n",
      "3      GCNConv   MGSSL    0.865205   0.038248         0.836682   \n",
      "4      GCNConv    None    0.835438   0.034938         0.814044   \n",
      "5      GCNConv  RBRICS    0.887925   0.053703         0.858162   \n",
      "6      GINConv   MGSSL    0.922676   0.019257         0.882940   \n",
      "7      GINConv    None    0.897334   0.029802         0.867652   \n",
      "8      GINConv  RBRICS    0.935077   0.009649         0.886514   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.031085   0.873411  0.028266  \n",
      "1        0.028628   0.858489  0.061453  \n",
      "2        0.032707   0.867643  0.037814  \n",
      "3        0.063130   0.867243  0.042326  \n",
      "4        0.063584   0.831402  0.033424  \n",
      "5        0.077961   0.865027  0.030767  \n",
      "6        0.043894   0.897349  0.026799  \n",
      "7        0.048032   0.870197  0.038328  \n",
      "8        0.033057   0.883030  0.037810  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_Lipophilicity.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.764187   0.010074         0.783406  \\\n",
      "1      GATConv    None    0.787272   0.011159         0.806845   \n",
      "2      GATConv  RBRICS    0.701027   0.017527         0.727413   \n",
      "3      GCNConv   MGSSL    0.753192   0.018659         0.771786   \n",
      "4      GCNConv    None    0.765452   0.008840         0.776981   \n",
      "5      GCNConv  RBRICS    0.702208   0.021252         0.723986   \n",
      "6      GINConv   MGSSL    0.689938   0.054496         0.726783   \n",
      "7      GINConv    None    0.696613   0.016007         0.732164   \n",
      "8      GINConv  RBRICS    0.640996   0.034889         0.707956   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.045848   0.782234  0.035812  \n",
      "1        0.039165   0.792182  0.044449  \n",
      "2        0.049688   0.726563  0.042137  \n",
      "3        0.045907   0.764276  0.042401  \n",
      "4        0.036412   0.760317  0.036280  \n",
      "5        0.040157   0.720102  0.047686  \n",
      "6        0.068407   0.752418  0.075850  \n",
      "7        0.029852   0.732894  0.042002  \n",
      "8        0.048325   0.708468  0.032933  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_Mutagenicity.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.837292   0.007163         0.797144  \\\n",
      "1      GATConv    None    0.765197   0.027184         0.734550   \n",
      "2      GATConv  RBRICS    0.859868   0.015472         0.807930   \n",
      "3      GCNConv   MGSSL    0.798012   0.007437         0.760964   \n",
      "4      GCNConv    None    0.775733   0.024900         0.750212   \n",
      "5      GCNConv  RBRICS    0.864932   0.007800         0.817602   \n",
      "6      GINConv   MGSSL    0.877189   0.004663         0.839923   \n",
      "7      GINConv    None    0.868088   0.006214         0.840009   \n",
      "8      GINConv  RBRICS    0.897783   0.003846         0.850584   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.022253   0.804340  0.020391  \n",
      "1        0.033346   0.760860  0.043071  \n",
      "2        0.031838   0.809319  0.026908  \n",
      "3        0.015118   0.775669  0.015347  \n",
      "4        0.030337   0.769721  0.038042  \n",
      "5        0.028893   0.815088  0.019176  \n",
      "6        0.007671   0.847497  0.014378  \n",
      "7        0.015567   0.848049  0.010086  \n",
      "8        0.018303   0.853890  0.015623  \n",
      "Grouped results saved to: ../kdd_results/results_summary_grouped_hERG.csv\n",
      "  Architecture    Type  Train_Mean  Train_Std  Validation_Mean   \n",
      "0      GATConv   MGSSL    0.760739   0.000525         0.744460  \\\n",
      "1      GATConv    None    0.720091   0.021565         0.704992   \n",
      "2      GATConv  RBRICS    0.809561   0.009122         0.768616   \n",
      "3      GCNConv   MGSSL    0.751258   0.003469         0.736260   \n",
      "4      GCNConv    None    0.715811   0.017676         0.703068   \n",
      "5      GCNConv  RBRICS    0.806743   0.013380         0.768262   \n",
      "6      GINConv   MGSSL    0.801573   0.009647         0.764613   \n",
      "7      GINConv    None    0.794640   0.016909         0.765167   \n",
      "8      GINConv  RBRICS    0.845046   0.008092         0.789779   \n",
      "\n",
      "   Validation_Std  Test_Mean  Test_Std  \n",
      "0        0.001659   0.737168  0.009483  \n",
      "1        0.027826   0.718262  0.035095  \n",
      "2        0.011557   0.765159  0.017628  \n",
      "3        0.019570   0.743297  0.017311  \n",
      "4        0.014009   0.716890  0.018092  \n",
      "5        0.009245   0.767559  0.017550  \n",
      "6        0.009465   0.764928  0.010393  \n",
      "7        0.012379   0.764843  0.015467  \n",
      "8        0.007003   0.776424  0.020230  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pdb\n",
    "\n",
    "# root_dir = \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/58\"\n",
    "# root_dirs_dict = {\"Lipophilicity\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/60\",\n",
    "#                               \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/61\",\n",
    "#                               \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/63\"\n",
    "#                              ],\n",
    "#                       \"esol\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/60\",\n",
    "#                              \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/62\"]}\n",
    "\n",
    "\n",
    "# Architectures and types to check\n",
    "architectures = [\"GATConv\", \"GINConv\", \"GCNConv\"]\n",
    "types = [\"MGSSL\", \"RBRICS\",\"None\"]\n",
    "\n",
    "# Folder name pattern: EXPT-{number}R-{dataset}-{seed}-{fold}-{architecture}-{type}\n",
    "folder_pattern = re.compile(\n",
    "    r\"EXPT-\\d+[a-zA-Z]+-[a-zA-Z]+-SEED-\\d+-FOLD-\\d+-(GATConv|GINConv|GCNConv)-[^-]+-(MGSSL|RBRICS)\"\n",
    ")\n",
    "\n",
    "folder_pattern_vanilla = re.compile(\n",
    "    r\"EXPT-\\d+[a-zA-Z]+-[a-zA-Z]+-SEED-\\d+-FOLD-\\d+-(GATConv|GINConv|GCNConv)-[^-]+-(None)?\"\n",
    ")\n",
    "\n",
    "for dataset_name, list_dirs in root_dirs_dict.items():\n",
    "    # Initialize the results dictionary\n",
    "    results = {\n",
    "        \"Architecture\": [],\n",
    "        \"Type\": [],\n",
    "        \"Train Metric\": [],\n",
    "        \"Validation Metric\": [],\n",
    "        \"Test Metric\": [],\n",
    "    }\n",
    "    \n",
    "\n",
    "    for root_dir in list_dirs:\n",
    "        # Traverse the directory and process matching folders\n",
    "        for folder in os.listdir(root_dir):\n",
    "            if folder_pattern.match(folder) or folder_pattern_vanilla.match(folder):\n",
    "                # Extract architecture and type from the folder name\n",
    "                match = folder_pattern.match(folder)\n",
    "                if match is not None:\n",
    "                    architecture, model_type = match.groups()\n",
    "                else:\n",
    "                    match = folder_pattern_vanilla.match(folder)\n",
    "                    architecture, model_type = match.groups()\n",
    "\n",
    "                if architecture in architectures and model_type in types:\n",
    "                    # Check for the JSON file\n",
    "                    file_path = os.path.join(root_dir, folder, f\"{dataset_name}_classification_result.json\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        with open(file_path, \"r\") as f:\n",
    "                            data = json.load(f)\n",
    "                        \n",
    "\n",
    "                        # Determine the appropriate metrics based on dataset_name\n",
    "                        if dataset_name in [\"BBBP\",\"Mutagenicity\",\"hERG\"]:\n",
    "                            train_metric = data.get(\"Trained_explainations_train_rocauc\", 0)\n",
    "                            validation_metric = data.get(\"Trained_explainations_validation_rocauc\", 0)\n",
    "                            test_metric = data.get(\"Trained_explainations_test_rocauc\", 0)\n",
    "                        else:\n",
    "                            train_metric = data.get(\"Trained_explainations_train_rmse\", 0)\n",
    "                            validation_metric = data.get(\"Trained_explainations_validation_rmse\", 0)\n",
    "                            test_metric = data.get(\"Trained_explainations_test_rmse\", 0)\n",
    "\n",
    "                        # Append single values for the current folder\n",
    "                        results[\"Architecture\"].append(architecture)\n",
    "                        results[\"Type\"].append(model_type)\n",
    "                        results[\"Train Metric\"].append(train_metric)\n",
    "                        results[\"Validation Metric\"].append(validation_metric)\n",
    "                        results[\"Test Metric\"].append(test_metric)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    df.to_csv(f\"../kdd_results/results_summary_{dataset_name}.csv\", index=False)\n",
    "\n",
    "    # Group by Architecture and Type, calculate mean and standard deviation\n",
    "    grouped = df.groupby([\"Architecture\", \"Type\"]).agg(\n",
    "        Train_Mean=(\"Train Metric\", \"mean\"),\n",
    "        Train_Std=(\"Train Metric\", \"std\"),\n",
    "        Validation_Mean=(\"Validation Metric\", \"mean\"),\n",
    "        Validation_Std=(\"Validation Metric\", \"std\"),\n",
    "        Test_Mean=(\"Test Metric\", \"mean\"),\n",
    "        Test_Std=(\"Test Metric\", \"std\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = f\"../kdd_results/results_summary_grouped_{dataset_name}.csv\"\n",
    "    grouped.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(\"Grouped results saved to:\", output_csv)\n",
    "    print(grouped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d11446b-da9f-40de-bd99-98b22832c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dirs_dict = {\"esol\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/esol/65\"],\n",
    "#                  \"BBBP\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/BBBP/64\"],\n",
    "#                  \"Lipophilicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Lipophilicity/66\"],\n",
    "#                  \"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\"]}\n",
    "\n",
    "\n",
    "root_dirs_dict = {\"esol\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/esol/65\"],\n",
    "                 \"BBBP\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/BBBP/64\"],\n",
    "                 \"Lipophilicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Lipophilicity/66\"],\n",
    "                 \"Mutagenicity\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/Mutagenicity/73\"],\n",
    "                  \"hERG\":[\"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/KDD/hERG/69\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb15cf-e344-4fd9-8bba-102920553a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF saved: ../kdd_results/esol_counts_Falseplots.pdf\n"
     ]
    }
   ],
   "source": [
    "# Show casing the importanve vs logit diff per epoch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pdb\n",
    "import torch\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from DataLoader import get_setup_files_with_folds\n",
    "\n",
    "# root_dir = \"/nfs/stak/users/kokatea/hpc-share/ChemIntuit/Cluster_JOBS/Regression/58\"\n",
    "# root_dirs_dict = {\"Lipophilicity\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/61\",\n",
    "#                               \"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/63\"\n",
    "#                              ],\n",
    "#                       \"esol\":[\"/nfs/stak/users/kokatea/ondemand/data/sys/myjobs/projects/default/62\"]}\n",
    "\n",
    "# Architectures and types to check\n",
    "architectures = [\"GATConv\", \"GINConv\", \"GCNConv\"]\n",
    "types = [\"MGSSL\", \"RBRICS\"]\n",
    "\n",
    "# Define folder name patterns\n",
    "folder_pattern = re.compile(\n",
    "    r\"EXPT-\\d+[a-zA-Z]+-[a-zA-Z]+-SEED-(\\d+)-FOLD-(\\d+)-(GATConv|GINConv|GCNConv)-[^-]+-(MGSSL|RBRICS)\"\n",
    ")\n",
    "\n",
    "date_tag = '1225'\n",
    "for use_count in [True,False]:\n",
    "\n",
    "    # Loop through datasets and directories\n",
    "    for dataset_name, list_dirs in root_dirs_dict.items():\n",
    "        pdf_path = f\"../kdd_results/{dataset_name}_counts_{use_count}plots.pdf\"\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            # root_dir = list_dirs[0]\n",
    "            for root_dir in list_dirs:\n",
    "                # Dictionary to track data availability\n",
    "                plot_data = {arch: {type_: {} for type_ in types} for arch in architectures}\n",
    "\n",
    "                # Traverse the directory and process matching folders\n",
    "                for folder in os.listdir(root_dir):\n",
    "                    if folder_pattern.match(folder):\n",
    "                        # Extract architecture, type, seed, and fold from the folder name\n",
    "                        match = folder_pattern.match(folder)\n",
    "                        seed, fold, architecture, model_type = match.groups()\n",
    "\n",
    "                        if architecture in architectures and model_type in types:\n",
    "                            # Check for the CSV file\n",
    "                            file_path = os.path.join(root_dir, folder, f\"explainer/{dataset_name}.csv\")\n",
    "                            if os.path.exists(file_path):\n",
    "                                # Read the CSV file\n",
    "                                df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "                                # Process `Logit Diff` and `Motif params`\n",
    "                                df['Logit Diff'] = df['Logit Diff'].apply(ast.literal_eval)\n",
    "                                df['Motif params'] = df['Motif params'].apply(lambda x: torch.sigmoid(torch.tensor(ast.literal_eval(x))).tolist())\n",
    "\n",
    "                                # Fetch `motif_lengths` from setup files\n",
    "                                _, _, motif_counts, motif_lengths, _, _, _, _, _, _, _ = get_setup_files_with_folds(dataset_name, date_tag, fold, model_type)\n",
    "\n",
    "\n",
    "\n",
    "                               # Store data for plotting by seed and fold\n",
    "                                plot_data[architecture][model_type].setdefault(seed, {})[fold] = (df, motif_lengths,motif_counts)\n",
    "\n",
    "\n",
    "                # Rename architectures\n",
    "                architecture_names = {\"GATConv\": \"GAT\", \"GINConv\": \"GIN\", \"GCNConv\": \"GCN\"}\n",
    "\n",
    "                # Iterate over seeds and folds for plotting\n",
    "                for seed in [0]:\n",
    "                    for fold in range(5):  # Assuming 0-4 folds\n",
    "                        # Set up the plot\n",
    "                        fig, axes = plt.subplots(2, len(architectures), figsize=(20, 10))\n",
    "                        fig.suptitle(f\"{dataset_name} - Seed {seed} Fold {fold}\", fontsize=16)\n",
    "                        plt.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "\n",
    "                        avg_motifs = {}\n",
    "\n",
    "                        # Plot MGSSL and RDBRICS on separate rows\n",
    "                        for i, arch in enumerate(architectures):\n",
    "                            for j, model_type in enumerate(types):\n",
    "                                ax = axes[j, i]\n",
    "                                # pdb.set_trace()\n",
    "                                plot_data_entry = plot_data[arch][model_type].get(str(seed), {}).get(str(fold), None)\n",
    "                                if plot_data_entry is not None:\n",
    "                                    df, motif_lengths,motif_counts = plot_data_entry\n",
    "                                    # pdb.set_trace()\n",
    "                                    num_motifs = len(df['Motif params'][0])\n",
    "                                    for motif_idx in range(num_motifs):\n",
    "                                        try:\n",
    "                                            # Extract start, trajectory, and end points\n",
    "                                            start_x, start_y = df['Motif params'][0][motif_idx], df['Logit Diff'][0][motif_idx]\n",
    "                                            end_x, end_y = df['Motif params'].iloc[-1][motif_idx], df['Logit Diff'].iloc[-1][motif_idx]\n",
    "                                            trajectory = [\n",
    "                                                (epoch[motif_idx], diff[motif_idx])\n",
    "                                                for epoch, diff in zip(df['Motif params'], df['Logit Diff']) if len(epoch) > motif_idx\n",
    "                                            ]\n",
    "                                            traj_x, traj_y = zip(*trajectory)\n",
    "\n",
    "                                            if use_count:\n",
    "                                                # Highlight start and end points with color based on motif length\n",
    "                                                motif_count = list(motif_counts.values())[motif_idx]\n",
    "                                                color = plt.cm.tab20c(motif_count / max(list(motif_counts.values())))\n",
    "                                            else:\n",
    "                                                # Highlight start and end points with color based on motif length\n",
    "                                                motif_length = list(motif_lengths.values())[motif_idx]\n",
    "                                                color = plt.cm.winter(motif_length / max(list(motif_lengths.values())))\n",
    "\n",
    "\n",
    "                                            # Plot trajectory\n",
    "                                            ax.plot(traj_x, traj_y, color=color, alpha=0.7)\n",
    "\n",
    "                                            # Ensure points are visible with a minimum size\n",
    "                                            point_size = 50\n",
    "\n",
    "                                            ax.scatter([start_x], [start_y], color='green', s=20)\n",
    "                                            ax.scatter([end_x], [end_y], color=color, s=20)\n",
    "\n",
    "                                            # Plot start and end points with color\n",
    "                                            # ax.scatter([start_x], [start_y], color=color, s=point_size, edgecolor='black', label=\"Start\")\n",
    "                                            # ax.scatter([end_x], [end_y], color=color, s=point_size, edgecolor='black', label=\"End\")\n",
    "\n",
    "                                            # ax.scatter([start_x], [start_y], color='green', s=point_size, label=\"Start\")\n",
    "                                            # ax.scatter([end_x], [end_y], color=color, s=point_size, edgecolor='black', label=\"End\")\n",
    "                                            # ax.scatter([start_x], [start_y], color='green', s=20, label=\"Start\")\n",
    "                                            # ax.scatter([end_x], [end_y], color='red', s=20, label=\"End\")\n",
    "\n",
    "                                        except (IndexError, KeyError):\n",
    "                                            continue\n",
    "\n",
    "                                    # Add epoch count annotation\n",
    "                                    num_epochs = len(df)\n",
    "                                    ax.text(0.35, 0.15, f\"Epochs: {num_epochs}\",\n",
    "                                            transform=ax.transAxes, fontsize=10,\n",
    "                                            ha='right', va='top', bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "                                    # Set plot labels\n",
    "                                    ax.set_title(f\"{architecture_names[arch]} - {model_type}\", fontsize=12)\n",
    "                                    ax.set_xlabel(\"Motif Parameters\")\n",
    "                                    ax.set_ylabel(\"Old Logit - Logit after removal\")\n",
    "                                    ax.set_xlim(0, 1)  # Set x-axis between 0 and 1\n",
    "                                    ax.grid(True)\n",
    "                                else:\n",
    "                                    # Add blank plot for missing data\n",
    "                                    ax.set_title(f\"{architecture_names[arch]} - {model_type}\", fontsize=12)\n",
    "                                    ax.text(0.5, 0.5, \"No Data\", fontsize=16, ha='center', va='center', color='gray')\n",
    "                                    ax.set_xticks([])\n",
    "                                    ax.set_yticks([])\n",
    "                                    ax.set_frame_on(False)\n",
    "                        if use_count:\n",
    "                            # Add colorbar for end points (shared across plots)\n",
    "                            sm = plt.cm.ScalarMappable(cmap='tab20c', norm=plt.Normalize(vmin=min(list(motif_counts.values())), vmax=max(list(motif_counts.values()))))\n",
    "                            sm.set_array([])\n",
    "                            fig.colorbar(sm, ax=axes[:, :], orientation='vertical', label='Motif Frequency')\n",
    "                        else:\n",
    "                            # Add colorbar for end points (shared across plots)\n",
    "                            sm = plt.cm.ScalarMappable(cmap='winter', norm=plt.Normalize(vmin=min(list(motif_lengths.values())), vmax=max(list(motif_lengths.values()))))\n",
    "                            sm.set_array([])\n",
    "                            fig.colorbar(sm, ax=axes[:, :], orientation='vertical', label='Motif Length')\n",
    "\n",
    "\n",
    "                        # Save the figure to the PDF\n",
    "                        pdf.savefig(fig)\n",
    "                        plt.close(fig)\n",
    "                break\n",
    "\n",
    "        print(f\"PDF saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a13f590-98d9-47f1-a6b7-0a6e984e7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_results_summary(folder_path):\n",
    "    # Initialize a list to store processed data for all datasets\n",
    "    processed_data = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file matches the required pattern\n",
    "        if file_name.startswith(\"results_summary_grouped_\") and file_name.endswith(\".csv\"):\n",
    "            # Extract the dataset name from the file name\n",
    "            dataset_name = file_name[len(\"results_summary_grouped_\"):-len(\".csv\")]\n",
    "\n",
    "            # Read the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Replace 'None' with 'Vanilla' in the 'Type' column\n",
    "            df['Type'] = df['Type'].fillna('Vanilla')\n",
    "            \n",
    "            # Replace architecture values\n",
    "            df['Architecture'] = df['Architecture'].replace({\n",
    "                \"GATConv\": \"GAT\",\n",
    "                \"GINConv\": \"GIN\",\n",
    "                \"GCNConv\": \"GCN\"\n",
    "            })\n",
    "\n",
    "            # Format Train, Validation, and Test metrics to 0.3f precision\n",
    "            df['Train Metric'] = df['Train_Mean'].map(\"{:.3f}\".format) + \" ± \" + df['Train_Std'].map(\"{:.3f}\".format)\n",
    "            df['Validation Metric'] = df['Validation_Mean'].map(\"{:.3f}\".format) + \" ± \" + df['Validation_Std'].map(\"{:.3f}\".format)\n",
    "            df['Test Metric'] = df['Test_Mean'].map(\"{:.3f}\".format) + \" ± \" + df['Test_Std'].map(\"{:.3f}\".format)\n",
    "\n",
    "            # Keep only relevant columns\n",
    "            df = df[['Architecture', 'Type', 'Train Metric', 'Validation Metric', 'Test Metric']]\n",
    "\n",
    "            # Add the dataset name to the DataFrame\n",
    "            df.insert(0, 'Dataset Name', dataset_name)\n",
    "            \n",
    "            # Sort the DataFrame by 'Type' with 'Vanilla' first\n",
    "            df['Type'] = pd.Categorical(df['Type'], categories=['Vanilla', 'MGSSL','RBRICS'], ordered=True)\n",
    "            df['Architecture'] = pd.Categorical(df['Architecture'], categories=['GAT', 'GCN','GIN'], ordered=True)\n",
    "            \n",
    "            df = df.sort_values(by=['Dataset Name', 'Architecture', 'Type'])\n",
    "            \n",
    "            \n",
    "\n",
    "            # Append to the processed data list\n",
    "            processed_data.append(df)\n",
    "\n",
    "    # Concatenate all processed data into a single DataFrame\n",
    "    final_df = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    # Split the data into two groups based on the dataset\n",
    "    group_1_datasets = [\"esol\", \"Lipophilicity\"]\n",
    "    group_2_datasets = [\"Mutagenicity\", \"BBBP\", \"hERG\"]\n",
    "\n",
    "    group_1_df = final_df[final_df['Dataset Name'].isin(group_1_datasets)].copy()\n",
    "    group_2_df = final_df[final_df['Dataset Name'].isin(group_2_datasets)].copy()\n",
    "\n",
    "    # Function to generate LaTeX table for a group\n",
    "    def generate_latex_table(dataframe, output_file):\n",
    "        # Remove duplicate dataset names for multirow formatting\n",
    "        dataframe.loc[:, 'Dataset Name'] = dataframe['Dataset Name'].mask(dataframe['Dataset Name'].duplicated(), '')\n",
    "\n",
    "        # Convert DataFrame to LaTeX table\n",
    "        latex_code = dataframe.to_latex(index=False, escape=False, column_format=\"|l|l|l|l|l|l|\", header=[\n",
    "            \"Dataset Name\", \"GNN Architecture\", \"Model\", \"Train Metric\", \"Validation Metric\", \"Test Metric\"\n",
    "        ])\n",
    "        \n",
    "        # Add table borders and formatting\n",
    "        latex_code = latex_code.replace('\\\\toprule', '\\\\hline').replace('\\\\midrule', '').replace('\\\\bottomrule', '\\\\hline')\n",
    "\n",
    "        # Write the LaTeX code to a file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(latex_code)\n",
    "\n",
    "        print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "    # Generate LaTeX tables for the two groups\n",
    "    group_1_file = os.path.join(folder_path, \"results_summary_regression.tex\")\n",
    "    group_2_file = os.path.join(folder_path, \"results_summary_binary_classification.tex\")\n",
    "\n",
    "    generate_latex_table(group_1_df, group_1_file)\n",
    "    generate_latex_table(group_2_df, group_2_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9232678-656a-4f4d-b918-fb13f5f2a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to ../kdd_results/results_summary_regression.tex\n",
      "LaTeX table saved to ../kdd_results/results_summary_binary_classification.tex\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = \"../kdd_results\"  # Replace with the path to your folder\n",
    "process_results_summary(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f8fe0-cc2e-43e1-a2eb-9722a4a6f913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-test",
   "language": "python",
   "name": "py36-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
